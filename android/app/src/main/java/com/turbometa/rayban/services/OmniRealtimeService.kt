package com.turbometa.rayban.services

import android.content.Context
import android.graphics.Bitmap
import android.media.AudioAttributes
import android.media.AudioFormat
import android.media.AudioManager
import android.media.AudioRecord
import android.media.AudioTrack
import android.media.MediaRecorder
import android.util.Base64
import android.util.Log
import com.google.gson.Gson
import com.google.gson.JsonObject
import com.turbometa.rayban.R
import com.turbometa.rayban.utils.AIProvider
import kotlinx.coroutines.*
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import okhttp3.*
import java.io.ByteArrayOutputStream
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.util.concurrent.TimeUnit

/**
 * Alibaba Qwen Omni Realtime Service
 * Supports multi-region endpoints (Beijing/Singapore)
 * 1:1 port from iOS OmniRealtimeService.swift
 */
class OmniRealtimeService(
    private val context: Context,
    private val apiKey: String,
    private val providerConfig: ProviderEndpoints,
    private val provider: AIProvider,
    private val outputLanguage: String = "zh-CN"
) {
    companion object {
        private const val TAG = "OmniRealtimeService"
        private const val SAMPLE_RATE = 24000
        private const val CHANNEL_CONFIG = AudioFormat.CHANNEL_IN_MONO
        private const val AUDIO_FORMAT = AudioFormat.ENCODING_PCM_16BIT
    }

    private val websocketURL: String
        get() = when (endpoint) {
            AlibabaEndpoint.BEIJING -> WS_BEIJING_URL
            AlibabaEndpoint.SINGAPORE -> WS_SINGAPORE_URL
        }

    // State
    private val _isConnected = MutableStateFlow(false)
    val isConnected: StateFlow<Boolean> = _isConnected

    private val _isRecording = MutableStateFlow(false)
    val isRecording: StateFlow<Boolean> = _isRecording

    private val _isSpeaking = MutableStateFlow(false)
    val isSpeaking: StateFlow<Boolean> = _isSpeaking

    private val _currentTranscript = MutableStateFlow("")
    val currentTranscript: StateFlow<String> = _currentTranscript

    private val _errorMessage = MutableStateFlow<String?>(null)
    val errorMessage: StateFlow<String?> = _errorMessage

    // Callbacks
    var onTranscriptDelta: ((String) -> Unit)? = null
    var onTranscriptDone: ((String) -> Unit)? = null
    var onUserTranscript: ((String) -> Unit)? = null
    var onSpeechStarted: (() -> Unit)? = null
    var onSpeechStopped: (() -> Unit)? = null
    var onError: ((String) -> Unit)? = null
    var onDebugMessage: ((String) -> Unit)? = null  // New debug callback

    // Internal
    private var webSocket: WebSocket? = null
    private var audioRecord: AudioRecord? = null
    private var audioTrack: AudioTrack? = null
    private var recordingJob: Job? = null
    private var audioPlaybackJob: Job? = null
    private val audioQueue = mutableListOf<ByteArray>()
    private val gson = Gson()
    private var scope = CoroutineScope(Dispatchers.IO + SupervisorJob())

    private var pendingImageFrame: Bitmap? = null
    
    // Provider detection
    private val isOpenAI: Boolean
        get() = providerConfig.wsBaseUrl.contains("openai", ignoreCase = true)

    private val client = OkHttpClient.Builder()
        .readTimeout(0, TimeUnit.MILLISECONDS)
        .build()

    fun connect() {
        if (_isConnected.value) return

        val url = "${providerConfig.wsBaseUrl}?model=${providerConfig.realtimeModel}"
        val request = Request.Builder()
            .url(url)
            .addHeader("Authorization", "Bearer $apiKey")
            .build()

        webSocket = client.newWebSocket(request, object : WebSocketListener() {
            override fun onOpen(webSocket: WebSocket, response: Response) {
                Log.d(TAG, "WebSocket connected")
                _isConnected.value = true
                sendSessionUpdate()
            }

            override fun onMessage(webSocket: WebSocket, text: String) {
                Log.d(TAG, "WebSocket message received: $text")
                // Show more for error/failed messages, less for normal messages
                val debugText = if (text.contains("\"error\"") || text.contains("\"failed\"")) {
                    text.take(800) // Show more for errors
                } else {
                    text.take(300) // Show more for normal messages too
                }
                onDebugMessage?.invoke("üì• RCV: $debugText")
                handleMessage(text)
            }

            override fun onFailure(webSocket: WebSocket, t: Throwable, response: Response?) {
                Log.e(TAG, "WebSocket error: ${t.message}")
                _isConnected.value = false
                _errorMessage.value = t.message
                onError?.invoke(t.message ?: context.getString(R.string.error_connection_failed))
            }

            override fun onClosed(webSocket: WebSocket, code: Int, reason: String) {
                Log.d(TAG, "WebSocket closed: $reason")
                _isConnected.value = false
            }
        })
    }

    fun disconnect() {
        stopRecording()
        stopAudioPlayback()
        webSocket?.close(1000, "User disconnected")
        webSocket = null
        _isConnected.value = false
        _isRecording.value = false
        _isSpeaking.value = false
        scope.cancel()
    }

    fun startRecording() {
        if (_isRecording.value) return

        try {
            val bufferSize = AudioRecord.getMinBufferSize(SAMPLE_RATE, CHANNEL_CONFIG, AUDIO_FORMAT)
            audioRecord = AudioRecord(
                MediaRecorder.AudioSource.MIC,
                SAMPLE_RATE,
                CHANNEL_CONFIG,
                AUDIO_FORMAT,
                bufferSize * 2
            )

            if (audioRecord?.state != AudioRecord.STATE_INITIALIZED) {
                Log.e(TAG, "Failed to initialize AudioRecord")
                return
            }

            audioRecord?.startRecording()
            _isRecording.value = true
            lastImageSentTime = 0  // ÈáçÁΩÆÔºåÁ°Æ‰øùÁ´ãÂç≥ÂèëÈÄÅÁ¨¨‰∏ÄÂº†ÂõæÁâá

            recordingJob = scope.launch {
                val buffer = ByteArray(bufferSize)
                while (isActive && _isRecording.value) {
                    val bytesRead = audioRecord?.read(buffer, 0, buffer.size) ?: 0
                    if (bytesRead > 0) {
                        sendAudioData(buffer.copyOf(bytesRead))
                    }
                }
            }
        } catch (e: SecurityException) {
            Log.e(TAG, "Microphone permission denied")
            _errorMessage.value = context.getString(R.string.error_microphone_permission)
        } catch (e: Exception) {
            Log.e(TAG, "Error starting recording: ${e.message}")
            _errorMessage.value = e.message
        }
    }

    fun stopRecording() {
        _isRecording.value = false
        recordingJob?.cancel()
        audioRecord?.stop()
        audioRecord?.release()
        audioRecord = null
    }

    fun updateVideoFrame(frame: Bitmap) {
        pendingImageFrame = frame
    }

    private fun sendSessionUpdate() {
        // Use mode manager if context is available, otherwise fall back to language-based prompt
        val instructions = context?.let {
            val modeManager = LiveAIModeManager.getInstance(it)
            modeManager.getSystemPrompt()
        } ?: getLiveAIPrompt(outputLanguage)

        val systemPrompt = when (outputLanguage) {
            "zh-CN" -> "‰Ω†ÊòØRayBan MetaÊô∫ËÉΩÁúºÈïúAIÂä©Êâã„ÄÇ$languageInstruction ÂõûÁ≠îË¶ÅÁÆÄÁªÉÔºåÈÄöÂ∏∏Âú®1-3Âè•ËØùÂÜÖÂÆåÊàê„ÄÇÂ¶ÇÊûúÁî®Êà∑ËØ¢ÈóÆ‰Ω†ÁúãÂà∞‰∫Ü‰ªÄ‰πàÔºåËØ∑ÊèèËø∞ËßÜËßâÁîªÈù¢‰∏≠ÁöÑÂÜÖÂÆπ„ÄÇ"
            "en-US" -> "You are the RayBan Meta smart glasses AI assistant. $languageInstruction Keep answers concise, typically 1-3 sentences. If the user asks what you see, describe the visual content."
            "ja-JP" -> "RayBan Meta„Çπ„Éû„Éº„Éà„Ç∞„É©„Çπ„ÅÆ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ$languageInstruction ÂõûÁ≠î„ÅØÁ∞°ÊΩî„Å´„ÄÅÈÄöÂ∏∏1„Äú3Êñá„Åß„ÄÇ„É¶„Éº„Ç∂„Éº„Åå‰Ωï„ÅåË¶ã„Åà„Çã„ÅãÂ∞ã„Å≠„Åü„Çâ„ÄÅË¶ñË¶öÁöÑ„Å™ÂÜÖÂÆπ„ÇíË™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
            "ko-KR" -> "RayBan Meta Ïä§ÎßàÌä∏ ÏïàÍ≤Ω AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. $languageInstruction ÎãµÎ≥ÄÏùÄ Í∞ÑÍ≤∞ÌïòÍ≤å, Î≥¥ÌÜµ 1-3Î¨∏Ïû•ÏúºÎ°ú. ÏÇ¨Ïö©ÏûêÍ∞Ä Î¨¥ÏóáÏù¥ Î≥¥Ïù¥ÎäîÏßÄ Î¨ºÏúºÎ©¥ ÏãúÍ∞ÅÏ†Å ÎÇ¥Ïö©ÏùÑ ÏÑ§Î™ÖÌïòÏÑ∏Ïöî."
            "es-ES" -> "Eres el asistente de IA de las gafas inteligentes RayBan Meta. $languageInstruction Mant√©n las respuestas concisas, t√≠picamente de 1-3 frases. Si el usuario pregunta qu√© ves, describe el contenido visual."
            "fr-FR" -> "Vous √™tes l'assistant IA des lunettes intelligentes RayBan Meta. $languageInstruction Gardez les r√©ponses concises, g√©n√©ralement 1-3 phrases. Si l'utilisateur demande ce que vous voyez, d√©crivez le contenu visuel."
            else -> "‰Ω†ÊòØRayBan MetaÊô∫ËÉΩÁúºÈïúAIÂä©Êâã„ÄÇ$languageInstruction ÂõûÁ≠îË¶ÅÁÆÄÁªÉÔºåÈÄöÂ∏∏Âú®1-3Âè•ËØùÂÜÖÂÆåÊàê„ÄÇÂ¶ÇÊûúÁî®Êà∑ËØ¢ÈóÆ‰Ω†ÁúãÂà∞‰∫Ü‰ªÄ‰πàÔºåËØ∑ÊèèËø∞ËßÜËßâÁîªÈù¢‰∏≠ÁöÑÂÜÖÂÆπ„ÄÇ"
        }

        // Build session configuration based on provider
        val sessionMap = mutableMapOf<String, Any>()

        // OpenAI Realtime API format
        if (isOpenAI) {
            sessionMap["type"] = "realtime"
            sessionMap["model"] = providerConfig.realtimeModel
            sessionMap["output_modalities"] = listOf("audio")
            sessionMap["instructions"] = systemPrompt
            sessionMap["audio"] = mapOf(
                "input" to mapOf(
                    "format" to mapOf(
                        "type" to "audio/pcm",
                        "rate" to 24000
                    ),
                    "turn_detection" to mapOf(
                        "type" to "server_vad",
                        "threshold" to 0.5,
                        "prefix_padding_ms" to 300,
                        "silence_duration_ms" to 500
                    )
                ),
                "output" to mapOf(
                    "format" to mapOf(
                        "type" to "audio/pcm",
                        "rate" to 24000
                    ),
                    "voice" to providerConfig.voice
                )
            )
        } else {
            // Alibaba Cloud format
            sessionMap["modalities"] = listOf("text", "audio")
            sessionMap["voice"] = providerConfig.voice
            sessionMap["instructions"] = systemPrompt
            sessionMap["input_audio_format"] = "pcm16"
            sessionMap["output_audio_format"] = "pcm16"
            sessionMap["smooth_output"] = true
            sessionMap["turn_detection"] = mapOf(
                "type" to "server_vad",
                "threshold" to 0.5,
                "silence_duration_ms" to 800
            )
        }

        val sessionConfig = mapOf(
            "type" to "session.update",
            "session" to sessionMap
        )

        val json = gson.toJson(sessionConfig)
        Log.d(TAG, "Sending session config: $json")
        onDebugMessage?.invoke("üì§ SEND: session.update")
        webSocket?.send(json)
    }

    private fun createResponse() {
        if (!_isConnected.value) return

        val responseConfig = mapOf(
            "type" to "response.create"
        )

        val json = gson.toJson(responseConfig)
        Log.d(TAG, "Requesting response creation: $json")
        onDebugMessage?.invoke("üì§ SEND: response.create")
        webSocket?.send(json)
    }

    /**
     * Get localized Live AI prompt matching iOS implementation
     */
    private fun getLiveAIPrompt(language: String): String {
        return when (language) {
            "zh-CN" -> """
                ‰Ω†ÊòØRayBan MetaÊô∫ËÉΩÁúºÈïúAIÂä©Êâã„ÄÇ

                „ÄêÈáçË¶Å„ÄëÂøÖÈ°ªÂßãÁªàÁî®‰∏≠ÊñáÂõûÁ≠îÔºåÊó†ËÆ∫Áî®Êà∑ËØ¥‰ªÄ‰πàËØ≠Ë®Ä„ÄÇ

                ÂõûÁ≠îË¶ÅÁÆÄÁªÉ„ÄÅÂè£ËØ≠ÂåñÔºåÂÉèÊúãÂèãËÅäÂ§©‰∏ÄÊ†∑„ÄÇÁî®Êà∑Êà¥ÁùÄÁúºÈïúÂèØ‰ª•ÁúãÂà∞Âë®Âõ¥ÁéØÂ¢ÉÔºåÊ†πÊçÆÁîªÈù¢Âø´ÈÄüÁªôÂá∫ÊúâÁî®ÁöÑÂª∫ËÆÆ„ÄÇ‰∏çË¶ÅÂï∞Âó¶ÔºåÁõ¥Êé•ËØ¥ÈáçÁÇπ„ÄÇ
            """.trimIndent()
            "en-US" -> """
                You are a RayBan Meta smart glasses AI assistant.

                [IMPORTANT] Always respond in English.

                Keep your answers concise and conversational, like chatting with a friend. The user is wearing glasses and can see their surroundings, provide quick and useful suggestions based on what they see. Be direct and to the point.
            """.trimIndent()
            "ja-JP" -> """
                „ÅÇ„Å™„Åü„ÅØRayBan Meta„Çπ„Éû„Éº„Éà„Ç∞„É©„Çπ„ÅÆ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ

                „ÄêÈáçË¶Å„ÄëÂ∏∏„Å´Êó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

                ÂõûÁ≠î„ÅØÁ∞°ÊΩî„Åß‰ºöË©±ÁöÑ„Å´„ÄÅÂèãÈÅî„Å®„ÉÅ„É£„ÉÉ„Éà„Åô„Çã„Çà„ÅÜ„Å´„ÄÇ„É¶„Éº„Ç∂„Éº„ÅØÁúºÈè°„Çí„Åã„Åë„Å¶Âë®Âõ≤„ÇíË¶ã„Å¶„ÅÑ„Åæ„Åô„ÄÇË¶ã„Åà„Çã„ÇÇ„ÅÆ„Å´Âü∫„Å•„ÅÑ„Å¶Á¥†Êó©„ÅèÊúâÁî®„Å™„Ç¢„Éâ„Éê„Ç§„Çπ„Çí„ÄÇË¶ÅÁÇπ„ÇíÁõ¥Êé•‰ºù„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
            """.trimIndent()
            "ko-KR" -> """
                ÎãπÏã†ÏùÄ RayBan Meta Ïä§ÎßàÌä∏ ÏïàÍ≤Ω AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§.

                „ÄêÏ§ëÏöî„ÄëÌï≠ÏÉÅ ÌïúÍµ≠Ïñ¥Î°ú ÏùëÎãµÌïòÏÑ∏Ïöî.

                ÏπúÍµ¨ÏôÄ ÎåÄÌôîÌïòÎìØÏù¥ Í∞ÑÍ≤∞ÌïòÍ≥† ÎåÄÌôîÏ†ÅÏúºÎ°ú ÎãµÎ≥ÄÌïòÏÑ∏Ïöî. ÏÇ¨Ïö©ÏûêÎäî ÏïàÍ≤ΩÏùÑ Ï∞©Ïö©ÌïòÍ≥† Ï£ºÎ≥ÄÏùÑ Î≥º Ïàò ÏûàÏäµÎãàÎã§. Î≥¥Ïù¥Îäî Í≤ÉÏóê Îî∞Îùº Îπ†Î•¥Í≥† Ïú†Ïö©Ìïú Ï°∞Ïñ∏ÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî. ÏöîÏ†êÎßå ÎßêÌïòÏÑ∏Ïöî.
            """.trimIndent()
            else -> getLiveAIPrompt("en-US")
        }
    }

    private fun sendAudioData(audioData: ByteArray) {
        if (!_isConnected.value) return

        val base64Audio = Base64.encodeToString(audioData, Base64.NO_WRAP)
        val message = mapOf(
            "type" to "input_audio_buffer.append",
            "audio" to base64Audio
        )

        webSocket?.send(gson.toJson(message))
        Log.v(TAG, "Sent audio data: ${audioData.size} bytes")

        // ÂÆöÊúüÂèëÈÄÅÂõæÁâáÔºàÊØè 500ms ÂèëÈÄÅ‰∏ÄÊ¨°Ôºâ
        val currentTime = System.currentTimeMillis()
        if (pendingImageFrame != null && (currentTime - lastImageSentTime >= imageSendIntervalMs)) {
            lastImageSentTime = currentTime
            sendImageFrame(pendingImageFrame!!)
        }
    }

    private fun sendImageFrame(bitmap: Bitmap) {
        try {
            if (isOpenAI) {
                // For OpenAI, analyze image with vision API and inject as context
                scope.launch {
                    try {
                        Log.d(TAG, "Analyzing image with GPT-4 Vision for OpenAI Realtime context")
                        onDebugMessage?.invoke("üì∑ Analyzing image...")
                        
                        val visionService = VisionAPIService(context, provider)
                        val result = visionService.analyzeImage(bitmap, "Describe what you see in this image briefly.")
                        
                        result.onSuccess { visionDescription ->
                            Log.d(TAG, "Vision context: $visionDescription")
                            onDebugMessage?.invoke("üëÅÔ∏è Vision: ${visionDescription.take(100)}")
                            
                            // Inject vision context as a system message in the conversation
                            val contextMessage = mapOf(
                                "type" to "conversation.item.create",
                                "item" to mapOf(
                                    "type" to "message",
                                    "role" to "system",
                                    "content" to listOf(
                                        mapOf(
                                            "type" to "input_text",
                                            "text" to "Context: User's current view shows: $visionDescription"
                                        )
                                    )
                                )
                            )
                            webSocket?.send(gson.toJson(contextMessage))
                            Log.d(TAG, "Vision context injected into conversation")
                        }.onFailure { error ->
                            Log.e(TAG, "Vision API error: ${error.message}")
                            onDebugMessage?.invoke("‚ùå Vision error: ${error.message}")
                        }
                    } catch (e: Exception) {
                        Log.e(TAG, "Error analyzing image: ${e.message}", e)
                    }
                }
                return
            }
            
            // Alibaba Cloud: Direct image buffer append
            val outputStream = ByteArrayOutputStream()
            bitmap.compress(Bitmap.CompressFormat.JPEG, 60, outputStream)
            val bytes = outputStream.toByteArray()
            val base64Image = Base64.encodeToString(bytes, Base64.NO_WRAP)

            val message = mapOf(
                "type" to "input_image_buffer.append",
                "image" to base64Image
            )

            webSocket?.send(gson.toJson(message))
            Log.d(TAG, "Image frame sent")
        } catch (e: Exception) {
            Log.e(TAG, "Error sending image: ${e.message}")
        }
    }

    private fun handleMessage(text: String) {
        try {
            val json = gson.fromJson(text, JsonObject::class.java)
            val type = json.get("type")?.asString ?: return
            
            Log.d(TAG, "Processing message type: $type")

            when (type) {
                "session.created", "session.updated" -> {
                    Log.d(TAG, "Session ready: $text")
                }
                "response.created" -> {
                    Log.d(TAG, "Response created")
                    onDebugMessage?.invoke("üéØ Response created")
                }
                "response.output_item.added" -> {
                    Log.d(TAG, "Output item added: $text")
                    onDebugMessage?.invoke("üìù Output item added")
                }
                "response.content_part.added" -> {
                    Log.d(TAG, "Content part added: $text")
                    onDebugMessage?.invoke("üìù Content part added")
                }
                "input_audio_buffer.speech_started" -> {
                    Log.d(TAG, "User speech started")
                    _isSpeaking.value = false
                    stopAudioPlayback()
                    onSpeechStarted?.invoke()
                }
                "input_audio_buffer.speech_stopped" -> {
                    Log.d(TAG, "User speech stopped")
                    onSpeechStopped?.invoke()
                    // Note: With server_vad turn_detection, OpenAI automatically creates a response
                    // No need to call response.create manually
                }
                "response.audio_transcript.delta" -> {
                    val delta = json.get("delta")?.asString ?: ""
                    Log.d(TAG, "Transcript delta: $delta")
                    _currentTranscript.value += delta
                    onTranscriptDelta?.invoke(delta)
                }
                "response.audio_transcript.done" -> {
                    val transcript = _currentTranscript.value
                    Log.d(TAG, "Transcript done: $transcript")
                    onTranscriptDone?.invoke(transcript)
                    _currentTranscript.value = ""
                }
                "conversation.item.input_audio_transcription.completed" -> {
                    val transcript = json.get("transcript")?.asString ?: ""
                    Log.d(TAG, "User transcript: $transcript")
                    onUserTranscript?.invoke(transcript)
                }
                // OpenAI event names
                "response.output_audio_transcript.delta" -> {
                    val delta = json.get("delta")?.asString ?: ""
                    Log.d(TAG, "Transcript delta (OpenAI): $delta")
                    _currentTranscript.value += delta
                    onTranscriptDelta?.invoke(delta)
                }
                "response.output_audio_transcript.done" -> {
                    val transcript = json.get("transcript")?.asString ?: _currentTranscript.value
                    Log.d(TAG, "Transcript done (OpenAI): $transcript")
                    onTranscriptDone?.invoke(transcript)
                    _currentTranscript.value = ""
                }
                "response.output_audio.delta" -> {
                    Log.d(TAG, "Audio delta event received (OpenAI), full JSON: $text")
                    onDebugMessage?.invoke("üîä Audio event (OpenAI): ${text.take(200)}")
                    
                    val audioData = json.get("delta")?.asString
                    if (audioData != null) {
                        val audioBytes = Base64.decode(audioData, Base64.DEFAULT)
                        Log.d(TAG, "Audio delta decoded: ${audioBytes.size} bytes")
                        onDebugMessage?.invoke("üîä Playing audio: ${audioBytes.size} bytes")
                        playAudio(audioBytes)
                    } else {
                        Log.w(TAG, "Audio delta is null in response.output_audio.delta")
                        onDebugMessage?.invoke("‚ö†Ô∏è Audio delta is null!")
                    }
                }
                "response.output_audio.done" -> {
                    Log.d(TAG, "Audio response done (OpenAI)")
                    _isSpeaking.value = false
                }
                // Alibaba Cloud event names
                "response.audio.delta" -> {
                    Log.d(TAG, "Audio delta event received (Alibaba), full JSON: $text")
                    onDebugMessage?.invoke("üîä Audio event (Alibaba): ${text.take(200)}")
                    
                    val audioData = json.get("delta")?.asString
                    if (audioData != null) {
                        val audioBytes = Base64.decode(audioData, Base64.DEFAULT)
                        Log.d(TAG, "Audio delta decoded: ${audioBytes.size} bytes")
                        onDebugMessage?.invoke("üîä Playing audio: ${audioBytes.size} bytes")
                        playAudio(audioBytes)
                    } else {
                        Log.w(TAG, "Audio delta is null in response.audio.delta")
                        onDebugMessage?.invoke("‚ö†Ô∏è Audio delta is null!")
                    }
                }
                "response.audio.done" -> {
                    Log.d(TAG, "Audio response done (Alibaba)")
                    _isSpeaking.value = false
                }
                "response.done" -> {
                    val response = json.get("response")?.asJsonObject
                    val status = response?.get("status")?.asString
                    Log.d(TAG, "Response done with status: $status")
                    
                    if (status == "failed") {
                        val statusDetails = response?.get("status_details")?.asJsonObject
                        val errorType = statusDetails?.get("type")?.asString
                        val errorObj = statusDetails?.get("error")?.asJsonObject
                        val errorCode = errorObj?.get("code")?.asString
                        val errorMessage = errorObj?.get("message")?.asString
                        
                        val fullError = "Response failed - Type: $errorType, Code: $errorCode, Message: $errorMessage"
                        Log.e(TAG, fullError)
                        onDebugMessage?.invoke("‚ùå ERROR: $fullError")
                        _errorMessage.value = errorMessage ?: context.getString(R.string.error_response_failed)
                        onError?.invoke(fullError)
                    }
                }
                "error" -> {
                    val errorMsg = json.get("error")?.asJsonObject?.get("message")?.asString
                    Log.e(TAG, "Server error: $errorMsg")
                    _errorMessage.value = errorMsg
                    onError?.invoke(errorMsg ?: "Unknown error")
                }
                else -> {
                    Log.d(TAG, "Unhandled message type: $type")
                    // Log all unhandled message types to debug
                    if (type.contains("audio") || type.contains("response")) {
                        onDebugMessage?.invoke("‚ö†Ô∏è Unhandled: $type")
                    }
                }
            }
        } catch (e: Exception) {
            Log.e(TAG, "Error handling message: ${e.message}", e)
        }
    }

    private fun playAudio(audioData: ByteArray) {
        Log.d(TAG, "playAudio called with ${audioData.size} bytes, queue size: ${audioQueue.size}")
        onDebugMessage?.invoke("üîä Queuing audio: ${audioData.size} bytes, queue: ${audioQueue.size}")
        synchronized(audioQueue) {
            audioQueue.add(audioData)
        }

        if (audioPlaybackJob?.isActive != true) {
            Log.d(TAG, "Starting audio playback")
            onDebugMessage?.invoke("‚ñ∂Ô∏è Starting audio playback")
            startAudioPlayback()
        }
    }

    private fun startAudioPlayback() {
        if (audioTrack == null) {
            val bufferSize = AudioTrack.getMinBufferSize(
                SAMPLE_RATE,
                AudioFormat.CHANNEL_OUT_MONO,
                AudioFormat.ENCODING_PCM_16BIT
            )

            // ‰ΩøÁî® AudioAttributes Êõø‰ª£Â∑≤ÂºÉÁî®ÁöÑ STREAM_MUSICÔºàÂÖºÂÆπÊÄßÊõ¥Â•ΩÔºâ
            val audioAttributes = AudioAttributes.Builder()
                .setUsage(AudioAttributes.USAGE_MEDIA)
                .setContentType(AudioAttributes.CONTENT_TYPE_SPEECH)
                .build()

            val audioFormat = AudioFormat.Builder()
                .setSampleRate(SAMPLE_RATE)
                .setChannelMask(AudioFormat.CHANNEL_OUT_MONO)
                .setEncoding(AudioFormat.ENCODING_PCM_16BIT)
                .build()

            audioTrack = AudioTrack(
                audioAttributes,
                audioFormat,
                bufferSize * 2,
                AudioTrack.MODE_STREAM,
                AudioManager.AUDIO_SESSION_ID_GENERATE
            )
            audioTrack?.play()
        }

        _isSpeaking.value = true

        audioPlaybackJob = scope.launch {
            while (isActive) {
                val data = synchronized(audioQueue) {
                    if (audioQueue.isNotEmpty()) audioQueue.removeAt(0) else null
                }

                if (data != null) {
                    // Directly write PCM16 data - no conversion needed
                    audioTrack?.write(data, 0, data.size)
                } else {
                    delay(10)
                    // Check if queue is still empty
                    val isEmpty = synchronized(audioQueue) { audioQueue.isEmpty() }
                    if (isEmpty) {
                        delay(100)
                        val stillEmpty = synchronized(audioQueue) { audioQueue.isEmpty() }
                        if (stillEmpty) {
                            _isSpeaking.value = false
                            break
                        }
                    }
                }
            }
        }
    }

    private fun stopAudioPlayback() {
        audioPlaybackJob?.cancel()
        synchronized(audioQueue) {
            audioQueue.clear()
        }
        audioTrack?.stop()
        audioTrack?.release()
        audioTrack = null
        _isSpeaking.value = false
    }

    private fun convertPcm24ToPcm16(pcm24Data: ByteArray): ByteArray {
        // PCM24 is 3 bytes per sample, PCM16 is 2 bytes per sample
        // We need to convert by taking the upper 16 bits of each 24-bit sample
        val sampleCount = pcm24Data.size / 3
        val pcm16Data = ByteArray(sampleCount * 2)
        val buffer = ByteBuffer.wrap(pcm24Data).order(ByteOrder.LITTLE_ENDIAN)
        val outBuffer = ByteBuffer.wrap(pcm16Data).order(ByteOrder.LITTLE_ENDIAN)

        for (i in 0 until sampleCount) {
            val sample24 = buffer.get().toInt() and 0xFF or
                    ((buffer.get().toInt() and 0xFF) shl 8) or
                    ((buffer.get().toInt() and 0xFF) shl 16)

            // Sign extend if negative
            val signedSample = if (sample24 and 0x800000 != 0) {
                sample24 or 0xFF000000.toInt()
            } else {
                sample24
            }

            // Take upper 16 bits
            val sample16 = (signedSample shr 8).toShort()
            outBuffer.putShort(sample16)
        }

        return pcm16Data
    }

    fun clearError() {
        _errorMessage.value = null
    }
}
